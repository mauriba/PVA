\setcounter{page}{1}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0pt} %obere Trennlinie

\section{Introduction}
Nowadays, many processes that handle large amounts of data (not to confuse with Big Data\footnote{Data
with high variety, volume and velocity. Cannot be processed by conventional data processing software.})
already exist. Still, they must be maintained, verified and optimized to ensure both
quality and efficiency.

\subsection{Motivation} \label{Motivation}
At q.beyond, we had to deal with just that: large monthly CSV exports
(each file contains the data of a month) by a third-party service provider
that our process was operating on. For verification, we collected the exports from a whole year,
grouped the records by a column and exported each group as an individual Excel file.
Each file contained two sheets: one with an aggregated monthly overview and the other with all the
individual records of that particular group.
% For this paper, we will focus on the aggregation.
In \ref{Grouping}, we will explain why those
two sheets require grouping, but only the latter requires every particular record to be stored
(in memory or storage).

% TODO: Result figure

We ended up writing a PowerShell\footnote{A scripting language coming with Windows. Used for administrative and automation tasks.}
Script, which, in our first draft, crashed the \gls{AVD}
it was running on due to insufficient memory.

\subsection{Main contributions}
Based on this experience, we can define the purpose of this paper:
\begin{itemize}
    \item \textbf{Problem}: We need to aggregate CSV data bigger than the available memory
    \item \textbf{Objectives}:
    \begin{enumerate}
        \item Find a solution that can group large CSV data
        \item Proof by tests and simulation that the solution works
    \end{enumerate}
    \item \textbf{Questions}:
    \begin{enumerate}
        \item Why does the \verb+Group-Object+ command not work on limited memory?
        \item What are requirements for a grouping algorithm in order to run on low memory?
    \end{enumerate}
\end{itemize}
As an alternative solution, we will consider MySQL, a \gls{DBMS}.

\newpage
\section{Background}

\subsection{Comma-Separated Values (CSV)}

Specified in \cite{rfc4180}, CSV is a file format for storing a table in plain text.
It has the following characteristics:

\begin{enumerate}
    \item each row starts on a new line
    \item the values for each column are separated by a delimiter (usually a comma)
    \item there may be a header line those values specify the names of the columns
    \item whitespaces other than line breaks for 1. are ignored, unless part of a value
\end{enumerate}
Our CSV export is structured in exactly this way. Let it be\footnote{
In this paper, we will use a simplification and abstraction of the original
data mentioned in \ref{Motivation}. The "$\dots$" indicates that
it is indeed a large CSV with many more records.}:

\begin{lstlisting}[escapeinside={(*}{*)},numbers=left,caption=CSV export,label={lst:csvexport}]
tid,    pname,  amount,     comment (*\label{csvexport:header}*)
1,      Alice,  -1,         daily expenses
2,      Bob,    2,          pocket money
3,      Alice,  1,          friend
4,      Carl,   0,          dummy
5,      Dan,    2.99,       card game
...
\end{lstlisting}

%TODO: Add csv export

\subsection{Relational Model}

Introduced by E. F. \cite{Codd70}, the relational model is a mathematical
description of data management, structuring data in tuples and tuples
in relations. A relational algebra is provided to act as a simple
mathematical query language. Industrial query languages
like SQL later implemented the features described in the relational model.

As further explained by \cite{Aren22}, a relation
is just a pair of a relation schema and a set of tuples (the rows of the relation).

\subsubsection{Relational Schema}
Arenas defines the relation schema based on three characteristics:
\begin{itemize}
    \item \textbf{relation name}: the name of the table
    \item \textbf{relation attributes}: a list of uniquely identifiable attributes/columns
    \item \textbf{relation arity}: the number of attributes/columns
\end{itemize}
\cite{Schw10} defines the relation schema more formally as a database-wide mapping
from relation names to either their respective arity (unnamed perspective) or
their respective attributes (named perspective). Then, a row in a relation
is either defined as a tuple of values $a = (a_1,\dots, a_n)$ (unnamed perspective) or
a set of pairs $a = \set{(c_1, a_1),\dots, (c_n, a_n)}$, where each pair consists of
the column name $c_n$ and the value $a_n$ assigned to it (named perspective).

In this paper, we will use relations as described in the named perspective.
For that, we must ensure that each column
got a unique name. Luckily, the header from our CSV export statisfies this condition
(see listing \ref{lst:csvexport}). Let its schema be:

\begin{center}
\verb+Transaction [ tid, pname, amount, comment ]+
\end{center}

Note that we do not assign any types to the attributes,
as the CSV header row only consists of attribute names.
This conforms with the understanding of the relational model
from Schweikardt, who just assumes that the values for our columns origin from an infinite set
containing any values for any columns.

The visual representation of a relation is a table. Below figure shows the
corresponding relation to the first 5 records of the CSV export from listing \ref{lst:csvexport}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        tid     & pname & amount    & comment \\ \hline\hline
        1       & Alice & -1        & daily expense \\ \hline
        2       & Bob   & 2         & pocket money \\ \hline
        3       & Alice & 1         & friend\\ \hline
        4       & Carl  & 0         & dummy\\ \hline
        5       & Dan   & 2.99      & card game\\ \hline
    \end{tabular}
    \caption{Transaction}
    \label{Transaction}
\end{table}

\subsubsection{Selection and Projection}

In chapter 12.1, \cite{Halp08} introduces relational algebra. It includes six
comparison operators $\theta \in \Set{=, <>, <, >, \leq, \geq}$ as well as the
three logical operators $\Set{\land, \lor, \neg}$. Furthermore, some
language-specific operations are defined. We will focus on the selection and projection.

Given a relation $R$, the selection operator $\sigma_c(R)$ selects all rows from
that relation that match the condition $c$ (similar to the \textbf{where} clause in \gls{SQL}).
Such condition can consist of any valid
expression using comparison operators, logical operators, attributes and constants.

For example, the selection $\sigma_{\text{pname}=\text{Alice}}(\text{Transaction})$
returns the following relation:

\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        tid     & pname & amount    & comment \\ \hline\hline
        1       & Alice & -1        & daily expense \\ \hline
        3       & Alice & 1         & friend\\ \hline
    \end{tabular}
    \caption{Selection of \textit{pname$=$Alice} on Transaction}
    \label{Selection of pname=Alice}
\end{table}

Similarly, the projection operator $\pi_{a1, \dots, a_n}(R)$ projects all columns
from a relation $R$ stated in the projection list $a1, \dots, a_n$, and
ignores all others. Consider the query $\pi_{\text{pname}}(\text{Transaction})$:

\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|}
        \hline
        pname \\ \hline\hline
        Alice \\ \hline
        Bob   \\ \hline
        Carl  \\ \hline
        Dan   \\ \hline
    \end{tabular}
    \caption{Projection of \textit{pname} on Transaction}
    \label{Projection of pname}
\end{table}

Note that the projection removes fully duplicate rows, hence why Alice only exists once in
the resulting relation.

As the visualizations suggests, both the selection and projection return a relation.

\subsection{Grouping} \label{Grouping}

Since a projection removes duplicated rows, projecting only
the grouping column (the column whose distinct values should be the name of our groups),
results in a relation containing all group names (see table \ref{Projection of pname}).

\subsubsection{Split into groups} \label{Split into groups}

We can use these group names to split the relation into distinct sub-relations consisting
only of the elements with the same group name:

\[
    G := \set{\sigma_{\text{pname}=x}(\text{Transaction})
        | (\text{pname}, x) \in \pi_{\text{pname}}(\text{Transaction})}
\]

with $G$ being a set of (sub)relations. We will refer to this kind of grouping as
\textbf{split into groups}. An algorithm which splits a relation into groups hence
must put every row into its respective group. In other words: a
split into groups requires all rows to be loaded in memory (or storage).

Apart from this observation, our definition of $G$ does not further specify the algorithm
a database could use to perform the split. Usually,
databases simply sort the relation on the grouping attribute, such that
rows belonging to the same group are next to each other.

\subsubsection{Aggregate on groups} \label{Aggregate on groups}

Oftentimes, we do not require all the details of all the rows in a group.
Instead, we only want a summary of our grouped data, also known as aggregation.
On p. 48, while listing features missing in basic relational algebra, \cite{Aren22}
find a great explanation for aggregation and grouping:

\begin{quote}
An extremely common feature of SQL queries
is the use of aggregation and grouping. Aggregation allows numerical functions to be
applied to entire columns, for example, to find the total salary
of all employees in a company. Grouping allows such columns to be split
according to a value of some attribute (...)
\end{quote}
They also formally define and analyze aggregation as an extension to relational algebra,
but we will instead use the (easier) aggregation operator presented in \cite{Elma89},
section 8.4.2. Let's describe our aggregation goal with it:

\[
    _{\text{pname}}\mathfrak{I}_{\text{SUM amount}}(\text{Transaction})
\]

$\mathfrak{I}$ is the aggregation operator, pname is the grouping column 
and SUM is the aggregation function we are using. For each group of
Transaction, the expression sums up the column "amount"
of all rows in that group.

Processing this statement could be, similar to \ref{Split into groups},
done by sorting the rows of Transaction by pname. One could, however,
think of other algorithms. For example:
store the name and current sum of each group while iterating over
the rows in Transaction. We can say that this expression
has more opportunity for being optimized for memory usage.

\subsection{External sorting} \label{External Sorting}

Usually, a database either sorts the columns or creates a hashtable
when eliminating duplicated rows or performing aggregation (\cite{Edga}).
Let's focus on the sorting for now. So if we want to aggregate on groups
with limited memory, we need to sort with limited RAM.

Sorting data bigger than the available memory can be achieved by buffering
temporary results to the storage. Such procedure is also known as external
sorting, since it uses external files to store part of its state. Usually,
a sort-merge strategy is used. \cite{Elma89} describe such an algorithm
in section 18.2. \\ \\
For our paper, there are two important conclusions:
\begin{enumerate}
    \item Aggregating on large data will most likely require sorting.
    \item We need external sorting strategies to sort data bigger than
the available memory.
\end{enumerate}
Most Relational \gls{DBMS} support external sorting. In our simulation,
we will see whether MySQL will use such kind of technique to perform
aggregation on large data.

\subsection{PowerShell's sorting algorithm}

So let's have a look at PowerShell's \verb+Group-Object+ command.
The Microsoft documentation \cite{docPSGO} for the Group-Object
command doesn't elaborate on the underlying algorithm used.
It does, however, reference the return type.
\verb+Group-Object+ either returns an instance of type
\verb+GroupInfo+ (see \cite{docPSGI})
or a \verb+hashtable+ (see \cite{docPSHT}).

Reading their documentations, what both these structures have in common
is that they are storing all elements from all groups in memory. This means,
\verb+Group-Object+ is splitting an input list into subgroups,
as defined in \ref{Split into groups}. The underlying data structures
don't make use of external sorting strategies either, as they are
holding the elements in memory. This is why PowerShell crashes when
trying to group data bigger than the available memory.

\newpage

\section{Simulation}

\subsection{Methods}

We use the following methods to find a solution that groups large CSV data:

\begin{enumerate}
    \item We started off with a \textbf{systematic literature research}
        to explain the bottleneck of our PowerShell implementation
        and find an alternative solution.
    \item We write a \textbf{test case}\footnote{We will use Pester,
        a PowerShell Testing library.} to ensure that the new
        MySQL implementation does indeed aggregate the data in the same way
        our PowerShell implementation did.
    \item We run a \textbf{simulation} of both solutions in a virtual
        environment limited in memory. We expect the MySQL implementation
        to run slowly but without crashes due to the external sorting
        strategy explained in \ref{External Sorting}.
\end{enumerate}

\subsection{Generating data}

For this paper, we need to implement a simple script which acts as the
service provider introduced in \ref{Motivation}. The script
should generate CSV data similar to the export we got from
the service provider. Note that:
\begin{itemize}
    \item The generated data must be large, large enough to exceed
        the memory.
    \item Nevertheless, the script itself should not crash from
        insufficient memory.
    \item Along with the generated data, the expected result of the
        aggregation should be provided for comparison.
\end{itemize}
For these reasons, we decided to let the script append a predefined
relation to the CSV file over and over again. We will use a large
text in the description fields to bloat the CSV file further.
You can find the script in the appendum.