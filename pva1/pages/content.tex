\setcounter{page}{1}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0pt} %obere Trennlinie

\section{Introduction}
Nowadays, many processes that handle large amounts of data (not to confuse with Big Data\footnote{Data
with high variety, volume and velocity. Cannot be processed by conventional data processing software.})
already exist. Still, they must be maintained, verified and optimized to ensure both
quality and efficiency.

\subsection{Motivation} \label{Motivation}
At q.beyond, we had to deal with just that: large monthly \gls{CSV} exports
(each file contains the data of a month) by a third-party service provider
that our process was operating on. For verification, we collected the exports from a whole year,
grouped their records by a column and exported each group as an individual Excel file.
Each file contained two sheets: one with an aggregated monthly overview and the other with all the
individual records of that particular group.
In \ref{Grouping}, we will explain why those
two sheets require grouping, but only the latter requires every particular group member to be stored
(in memory or storage).

We ended up writing a \gls{PowerShell}
Script, which, in our first draft, crashed the \gls{AVD}
it was running on due to insufficient memory.

\subsection{Main contributions}
Based on this experience, we can define the purpose of this paper:
\begin{itemize}
    \item \textbf{Problem:} We need to aggregate \gls{CSV} data bigger than the available memory
    \item \textbf{Objectives:}
    \begin{enumerate}
        \item Find a solution that can group large \gls{CSV} data
        \item Proof that the solution works by running an
            experiment with data bigger than the available memory
    \end{enumerate}
    \item \textbf{Questions:}
    \begin{enumerate}
        \item Why does the \verb+Group-Object+ command not work on limited memory?
        \item What are requirements for a grouping algorithm in order to run on low memory?
    \end{enumerate}
\end{itemize}
As an alternative solution, we will consider \gls{MySQL}, a \gls{DBMS}.

\newpage
\section{Background}

\subsection{Comma-Separated Values (CSV)}

Specified in \cite{rfc4180}, \gls{CSV} is a file format for storing a table in plain text.
It has the following characteristics:

\begin{enumerate}
    \item each row starts on a new line
    \item the values for each column are separated by a delimiter (usually a comma)
    \item there may be a header line those values specify the names of the columns
\end{enumerate}
Our \gls{CSV} export is structured in exactly this way. Let it be\footnote{
In this paper, we will use a simplification and abstraction of the original
data mentioned in \ref{Motivation}. The "$\dots$" indicate that
it is indeed a large \gls{CSV} with many more records.}:

\begin{lstlisting}[escapeinside={(*}{*)},numbers=left,caption=CSV export,label={lst:csvexport}]
tid,    pname,  amount,     comment (*\label{csvexport:header}*)
1,      Alice,  -1,         daily expenses
2,      Bob,    2,          pocket money
3,      Alice,  1,          friend
4,      Carl,   0,          dummy
5,      Dan,    2.99,       card game
...
\end{lstlisting}

\subsection{Relational Model}

Introduced by E. F. Codd in \cite{Codd70}, the relational model is a mathematical
description of data management, structuring data in tuples and tuples
in relations. A relational algebra is provided to act as a simple
mathematical query language. Industrial query languages
like SQL later implemented the features described in the relational model.

As further explained in \cite{Aren22}, a relation
is just a pair of a relation schema and a set of tuples (the rows of the relation).

\subsubsection{Relational Schema}
Arenas defines the relation schema based on three characteristics:
\begin{itemize}
    \item \textbf{relation name}: the name of the table
    \item \textbf{relation attributes}: a list of uniquely identifiable attributes/columns
    \item \textbf{relation arity}: the number of attributes/columns
\end{itemize}
\cite{Schw10} defines the relation schema more formally as a database-wide mapping
from relation names to either their respective arity (unnamed perspective) or
their respective attributes (named perspective). Then, a row in a relation
is either defined as a tuple of values $a = (a_1,\dots, a_n)$ (unnamed perspective) or
a set of pairs $a = \set{(c_1, a_1),\dots, (c_n, a_n)}$, where each pair consists of
the column name $c_n$ and the value $a_n$ assigned to it (named perspective).

In this paper, we will use relations as described in the named perspective.
For that, we must ensure that each column
got a unique name. Luckily, the header from our \gls{CSV} export statisfies this condition
(see listing \ref{lst:csvexport}). Let its schema be:

\begin{center} \label{schema}
\verb+Transaction [ tid, pname, amount, comment ]+
\end{center}

Note that we do not assign any types to the attributes,
as the \gls{CSV} header row only consists of attribute names.
This conforms with the understanding of the relational model
from Schweikardt, who just assumes that the values for our columns origin from an infinite set
containing any values for any columns.

The visual representation of a relation is a table. Below figure shows the
corresponding relation to the first 5 records of the \gls{CSV} export from listing \ref{lst:csvexport}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        tid     & pname & amount    & comment \\ \hline\hline
        1       & Alice & -1        & daily expense \\ \hline
        2       & Bob   & 2         & pocket money \\ \hline
        3       & Alice & 1         & friend\\ \hline
        4       & Carl  & 0         & dummy\\ \hline
        5       & Dan   & 2.99      & card game\\ \hline
    \end{tabular}
    \caption{Transaction}
    \label{Transaction}
\end{table}

\subsubsection{Selection and Projection}

In chapter 12.1, \cite{Halp08}
introduces relational algebra. It includes six
comparison operators $\theta \in \Set{=, <>, <, >, \leq, \geq}$ as well as the
three logical operators $\Set{\land, \lor, \neg}$. Furthermore, some
language-specific operations are defined. We will focus on the selection and projection.

Given a relation $R$, the selection operator $\sigma_c(R)$ selects all rows from
that relation that match the condition $c$ (similar to the \textbf{where} clause in \gls{SQL}).
Such condition can consist of any valid
expression using comparison operators, logical operators, attributes and constants.

For example, the selection $\sigma_{\text{pname}=\text{Alice}}(\text{Transaction})$
returns the following relation:

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        tid     & pname & amount    & comment \\ \hline\hline
        1       & Alice & -1        & daily expense \\ \hline
        3       & Alice & 1         & friend\\ \hline
    \end{tabular}
    \caption{Selection of \textit{pname$=$Alice} on Transaction}
    \label{Selection of pname=Alice}
\end{table}

Similarly, the projection operator $\pi_{a1, \dots, a_n}(R)$ projects all columns
from a relation $R$ stated in the projection list $a1, \dots, a_n$, and
ignores all others. Consider the query $\pi_{\text{pname}}(\text{Transaction})$:

\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|}
        \hline
        pname \\ \hline\hline
        Alice \\ \hline
        Bob   \\ \hline
        Carl  \\ \hline
        Dan   \\ \hline
    \end{tabular}
    \caption{Projection of \textit{pname} on Transaction}
    \label{Projection of pname}
\end{table}

Note that the projection removes fully duplicate rows, hence why Alice only exists once in
the resulting relation.

As the visualizations suggests, both the selection and projection return a relation.

\subsection{Grouping} \label{Grouping}

Since a projection removes duplicated rows, projecting only
the grouping column (the column whose distinct values should be the name of our groups),
results in a relation containing all group names (see table \ref{Projection of pname}).

\subsubsection{Split into groups} \label{Split into groups}

We can use these group names to split the relation into distinct sub-relations consisting
only of the elements with the same group name

\[
    G := \set{\sigma_{\text{pname}=x}(\text{Transaction})
        | (\text{pname}, x) \in \pi_{\text{pname}}(\text{Transaction})}
\]

being a set of (sub)relations. We will refer to this kind of grouping as
\textbf{split into groups}. An algorithm which splits a relation into groups hence
must put every row into its respective group. In other words: a
split into groups requires all rows to be loaded in memory (or storage).

Apart from this observation, our definition of $G$ does not further specify the algorithm
a database could use to perform the split. Usually,
databases simply sort the relation on the grouping attribute, such that
rows belonging to the same group are next to each other.

\subsubsection{Aggregate on groups} \label{Aggregate on groups}

Oftentimes, we do not require all the details of all the rows in a group.
Instead, we only want a summary of our grouped data, also known as aggregation.
While listing features missing in basic relational algebra, \cite{Aren22}
provides a great explanation for aggregation and grouping:

\begin{quote}
An extremely common feature of SQL queries
is the use of aggregation and grouping. Aggregation allows numerical functions to be
applied to entire columns, for example, to find the total salary
of all employees in a company. Grouping allows such columns to be split
according to a value of some attribute (...)
\end{quote}
They also formally define and analyze aggregation as an extension to relational algebra,
but we will instead use the (easier) aggregation operator presented in \cite{Elma89},
section 8.4.2. Let's describe our aggregation goal with it:

\[
    _{\text{pname}}\mathfrak{I}_{\text{SUM amount}}(\text{Transaction})
\]

$\mathfrak{I}$ is the aggregation operator, \verb+pname+ is the grouping column 
and \verb+SUM+ is the aggregation function we are using. For each group of
Transaction, the expression sums up the column \verb+amount+
of all rows in that group.

Processing this statement could be, similar to \ref{Split into groups},
done by sorting the rows of Transaction by pname. One could, however,
think of other algorithms. For example:
store the name and current sum of each group while iterating over
the rows in Transaction.
We see that this expression has more opportunity for being optimized.

\subsection{External sorting} \label{External Sorting}

Usually, a database either sorts the columns or creates a hashtable
when eliminating duplicated rows or performing aggregation (see \cite{Edga}).
Let's focus on the sorting for now. So if we want to aggregate on groups
with limited \gls{RAM}, we need to sort with limited \gls{RAM}.

Sorting data bigger than the available memory can be achieved by buffering
temporary results to the storage. Such procedure is also known as external
sorting, since it uses external files to store part of its state. Usually,
a sort-merge strategy is used. \cite{Elma89} describes such an algorithm
in section 18.2. \\ \\
For our paper, there are two important conclusions:
\begin{enumerate}
    \item Aggregating on large data will most likely require sorting.
    \item We need external sorting strategies to sort data bigger than
the available memory.
\end{enumerate}
Most Relational \gls{DBMS} support external sorting. In our simulation,
we will see that \gls{MySQL} will use such kind of technique to perform
aggregation on large data.

\subsection{PowerShell's sorting algorithm} \label{PowerShell Sorting}

So let's have a look at \gls{PowerShell}'s \verb+Group-Object+ command.
The Microsoft documentation \cite{docPSGO} for the Group-Object
command doesn't elaborate on the underlying algorithm used.
It does, however, reference the return type.
\verb+Group-Object+ either returns an instance of type
\verb+GroupInfo+ (see \cite{docPSGI})
or a \verb+hashtable+ (see \cite{docPSHT}).

Reading their documentations, what both these structures have in common
is that they are storing all elements from all groups in memory. This means,
\verb+Group-Object+ is splitting an input list into subgroups,
as defined in \ref{Split into groups}. The underlying data structures
do not make use of external sorting strategies either, as they are
holding the elements in memory. This is why \gls{PowerShell} crashes when
trying to group data bigger than the available memory.

\newpage

\section{Experiment}

\subsection{Methods}

We use the following methods to find a solution that groups large \gls{CSV} data:

\begin{enumerate}
    \item We started off with a \textbf{systematic literature research}
        to explain the bottleneck of our \gls{PowerShell} implementation
        and find an alternative solution.
    \item We build a \textbf{test}\footnote{For simplification,
        we just compare the aggregation result from both
        implementations on the same test data} to ensure that the new
        \gls{MySQL} implementation does indeed aggregate the data in the same way
        our \gls{PowerShell} implementation did.
    \item We run an \textbf{experiment} where both solutions run in
        \gls{Docker} containers limited in memory. We expect the \gls{MySQL} implementation
        to run slowly but without crashes due to the external sorting
        strategy explained in \ref{External Sorting}.
\end{enumerate}

\subsection{Generating records} \label{Generating records}

For this paper, we need to implement simple scripts which acts as the
service provider introduced in \ref{Motivation}. The scripts
should generate data similar to the export we got from
the service provider. Note that:
\begin{itemize}
    \item The generated data must be large, large enough to exceed
        the memory.
    \item Nevertheless, the script itself should not crash from
        insufficient memory.
    \item Along with the generated data, the expected result from the
        aggregation should be provided for comparison.
\end{itemize}
For these reasons, we decided to let the scripts append the
sample relation \ref{Transaction} to the \gls{CSV} file / \gls{MySQL} \gls{DB}
over and over again. We will use a large text in the description
fields to bloat the records further.

The scripts work by repeating the sample relation 400 times in memory,
then writing this chunk of data to a \gls{CSV}/\gls{SQL} file for another 200 times.
In total, we get $5*400*200=400,000$ records, which are about 403.25MB
of record data. Splitting the workload into multiple \gls{I/O} operations
ensures that we can generate all this data on very limited memory.

For more details, see the scripts in the appendix \ref{Appendix}.

\subsection{Using Docker}

As explained on \gls{Docker}'s website \cite{docContainer}, a container isolates
an application from our operating system and files.
It bundles the application source code and
required dependencies together, which makes the application
very portable as long as we got a software which can run our container.

We will use \gls{Docker}\footnote{More precisely, the Docker Engine, an open-source containerization software.}
for that. Both our \gls{PowerShell} and MySQL implementations
will be created and run as \gls{Docker} containers. Thus,
the experiment will execute similar on different hosts, making
it easy for others to repeat.

\gls{Docker} uses so-called Dockerfiles to define the parts of a container.

\subsection{PowerShell Implementation}

Let us have a look at the Dockerfile for our \gls{PowerShell} implementation.

\begin{lstlisting}[escapeinside={(*}{*)},numbers=left,caption=PS Dockerfile,label={lst:psdocker}]
FROM mcr.microsoft.com/PowerShell:lts-7.2-ubuntu-20.04 (*\label{psdocker:image}*)
COPY . / (*\label{psdocker:copy}*)
RUN pwsh /Generate-CSVExport.ps1 (*\label{psdocker:gencsv}*)
ENTRYPOINT [ "/docker-entrypoint.sh" ] (*\label{psdocker:entrypoint}*)
\end{lstlisting}
The container for our \gls{PowerShell} implementation needs
a \gls{PowerShell} installation. We use the \gls{PowerShell} Ubuntu package \cite{dockerPS}
as a base image (line \ref{psdocker:image})
and copy two \gls{PowerShell} scripts into our container (line \ref{psdocker:copy}).

The first script is executed when the container image is created
(line \ref{psdocker:gencsv}). It generates the large \gls{CSV}
file as described in \ref{Generating records}. Note that we
decided to ignore the \verb+tid+ attribute, as we do not
need a Primary Key in \gls{CSV}.

When the container is running, the second script
(line \ref{psdocker:entrypoint}) will be executed:

\begin{lstlisting}[language={PowerShell},escapeinside={(*}{*)},numbers=left,caption=aggregate.ps1,label={lst:psdocker}]
$csv = Import-CSV "./export.csv" (*\label{psagg:import}*)
$groups = $csv | Group-Object -Property pname
$summary = @{}

foreach ($group in $groups) {
    $summary[$group.Name] = 0
    foreach ($record in $group.Group) {
        $summary[$group.Name] += [int] $record.amount
    }
}

$summary (*\label{psagg:out}*)
\end{lstlisting}
It imports all the \gls{CSV} data (line \ref{psagg:import}) that we generated
before and uses \verb+Group-Object+ to group the records by their \verb+pname+.
By iterating over the groups and their members, the script calculates
the sum of amount for all members of a group. The
aggregation is stored in a hashlist
\verb+{"group": <current_sum>}+ and printed out at in line \ref{psagg:out}.

We can see that \verb+Group-Object+ requires all records
to be loaded in memory. For large \gls{CSV} files, this is not efficient.

\subsection{MySQL Implementation}

So let us look at the \gls{MySQL} implementation:

\begin{lstlisting}[escapeinside={(*}{*)},numbers=left,caption=MySQL Dockerfile,label={lst:sqldocker}]
FROM ubuntu/mysql (*\label{sqldocker:image}*)
ENV MYSQL_ROOT_PASSWORD="root"
ENV MYSQL_DATABASE="grouping"
COPY . / (*\label{sqldocker:copy}*)
RUN /generate-db.sh (*\label{sqldocker:gensql}*)
\end{lstlisting}
Line \ref{sqldocker:image} references the \gls{MySQL} image \cite{dockerMySQL},
based off Ubuntu. We chose Ubuntu for both \gls{PowerShell} and \gls{MySQL} to
make the containers more comparable.

We also provide environment variables for the \gls{DBMS} root password
and database name. These variables will be used by the base image.

After copying the generation script, we execute it
(lines \ref{sqldocker:copy}-\ref{sqldocker:gensql}).
The script creates:
\begin{enumerate}
    \item An SQL file \path{a-init-schema.sql} for initializing the schema of our database.
        We use the schema defined in \ref{schema} and only add
        the mandatory data types \verb+VARCHAR+ and \verb+INT+.
        We chose the column tid as the primary key.
    \item A bunch of identical SQL files \path{b-init-rows-<i>.sql},
        all inserting the same
        rows into our database over and over again.
    \item A bash script \path{c-run-query.sh} which performs the
        aggregation query once the initialization is done.
\end{enumerate}
The aggregation query is the equivalent to the operation implemented in \gls{PowerShell},
summing up the \verb+amount+s for each group grouped by \verb+pname+:
\begin{lstlisting}[language={SQL},escapeinside={(*}{*)},numbers=left,caption=MySQL aggregation,label={lst:sqlagg}]
SELECT pname, SUM(amount)
    FROM Transaction
    GROUP BY pname
\end{lstlisting}
All files are written to \path{/docker-entrypoint-initdb.d}
and prefixed with a-c to specify the order of execution.
Like the documentation suggests, the \gls{MySQL} base image does then
run the scripts in alphabetical order once the \gls{DBMS}
has started. From here, it is easy to run the experiment.

\subsection{Running the experiment}

\paragraph{Preconditions:}
\gls{Docker} is installed. A terminal is opened in a directory
with all files needed for the \gls{PowerShell} or \gls{MySQL} implementation. \\\\
We will turn the Dockerfile into a \gls{Docker} image and use it to
run a container with these two commands:

\begin{lstlisting}[language={bash},escapeinside={(*}{*)},numbers=left,caption=Run experiment,label={lst:rundocker}]
docker build -t <tag> .
docker run --memory 1000M --memory-swap 1000M <tag>
\end{lstlisting}
Note that \verb+<tag>+ is just a tag for the image to easily reference it.
We could as well look into \gls{Docker} to find the id of the image and use that.

\verb+--memory+ limits the maximum amount of memory
a container can use. \verb+--memory+, on the other hand, also limits
the size of \gls{swap} files. By setting both parameters to the same value,
we ensure that there is no \gls{swap} file being used at all, thereby
enforcing memory constraints (see \cite{dockerMem}). 1000M (Megabytes) is a good start value
for our experiments: It is most likely lower than what the \gls{OS} along with the 403.52MB
\gls{CSV} export need to be loaded, but just enough for \gls{MySQL} to run and import
all the data.

Running the containers with enough memory results in the summary shown
below. Compare the container output with the following table to proof
that the implementation is working correctly:

\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        pname & SUM(amount) \\ \hline\hline
        Alice & $0$ \\ \hline
        Bob   & $160,000$ \\ \hline
        Carl  & $0$ \\ \hline
        Dan   & $240,000$ \\ \hline
    \end{tabular}
    \caption{Result of aggregation}
    \label{Result of aggregation}
\end{table}

\newpage

\section{Discussion}

\subsection{Evaluation} \label{Evaluation}

We have run the \gls{PowerShell} container four, the \gls{MySQL} container two times.
While that is not a representative number, the isolation of our experiment
within \gls{Docker} containers indicates that container crashes indeed occure
purely because of our application, or more precisely, because a given implementation
was not able to perform the grouping operation on such low \gls{RAM}.

Yet these experiments are just little indications and do neither measure
the probability of a grouping operation to succeed, nor the minimum amount
of memory needed to consistently finish the aggregation.

Moreover, as we have not looked at the underlying implementation or at least log
outputs from \gls{MySQL} and \gls{PowerShell}, these experiments do not fully proof our assumptions.

Precisely, we do not know if \gls{MySQL} actually used an external sorting algorithm
as explored in \ref{External Sorting}. We argue that \gls{MySQL} must have stored parts
of its calculation to the disk, because the available memory was \textit{probably}
lower than the $403.25$M the \gls{CSV} data consumes. But we have not measured
the memory usage and do not know how much the \gls{OS} and other processes actually occupied.

\subsection{Findings}

Running the \gls{PowerShell} implementation yielded the following results:

\begin{itemize}
    \item \textbf{Import CSV:} The aggregation crashes on 1000MB and 1500MB when
        trying to import the whole \gls{CSV} file.

    \item \textbf{Group-Object:} The aggregation crashes on 1700MB and works on 1800MB
    when trying to group the records imported before.
\end{itemize}
Not only did we proof that \gls{PowerShell} needs a wasteful amount of memory to
import the whole \gls{CSV} file. If we compare the numbers of 1500MB (when the import
still crashed) and 1800MB (when the grouping succeeded), we can also see that
\verb+Group-Object+ has less than $1800-1500=300$MB of memory to perform.
These 300MB are less than the size of the \gls{CSV} file, but we found out
in \ref{PowerShell Sorting} that \gls{PowerShell}'S \verb+Group-Object+
returns either a \verb+GroupInfo+ or a \verb+hashlist+ containing all records.
How does this work?

While \gls{PowerShell} does copy the imported records into their respective group,
since they are objects, it does so \textbf{by reference} (see \cite{psRef}).
Copying by reference is a technique where only a reference to the object
itself is copied. Now both variables point to the same object, thus any change
to the copy would be reflected by the original. \\ \\
Compare this to the results of the \gls{MySQL} implementation:
\begin{itemize}
    \item \textbf{Startup:} Initialization of the DB and inserting 400,000
        records takes significantly more time.
    \item \textbf{Crash:} The implementation crashes at 700MB.
    \item \textbf{Success:} The implementation returns the correct sums at 1000MB.
\end{itemize}
Given that both implementations are based off Ubuntu, it is surprising
that \gls{MySQL} achieves aggregation with 1000MB. \gls{PowerShell} needs more
memory just to load the same dataset into memory. This is why we assume
that the dataset is larger than the available memory (it was for \gls{PowerShell}),
and that \gls{MySQL} manages this by using external sorting strategies to group the data. \\ \\
This experiment is a very basic example of grouping large data. Of course,
the data mentioned in \ref{Motivation} is far more complex, and so was
the aggregation process. Nevertheless, the principles of this experiment still apply:
simple grouping commands like \gls{PowerShell}'s \verb+Group-Object+ command
are not able to handle large data. Only by using external sorting strategies,
\gls{MySQL} manages to aggregate data much larger than the available memory.

\subsection{Future Work}
As explained in \ref{Evaluation}, the results from this experiment are not
certain. The experiment must be turned into a series of experiments in order
to find a statistic significance and a minimum amount of \gls{RAM} needed
for our grouping operation. Furthermore, monitoring tools could be used
to examine how much \gls{RAM} of the container is actually assigned to our
\gls{MySQL}/\gls{PowerShell} implementation.

Comparing a simple, versatile \gls{PowerShell} command to a full-featured \gls{DBMS}
is not fair. A future study could focus on more specialized libraries
or frameworks for grouping data in \gls{PowerShell}.

Usually, the analysis of large data is done for a company goal.
Communicating results and collaborating with other people
might require visualization. In fact, this is what our company is going to
do next: build a \gls{PowerBI} visialization of the data mentioned in \ref{Motivation}.